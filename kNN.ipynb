{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nikun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "from nltk.data import path\n",
    "# append your path for nltk data\n",
    "path.append(\"C:\\\\Users\\\\andri\\\\AppData\\\\Roaming\\\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sapmle.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m X, y2 \u001b[39m=\u001b[39m [], []\n\u001b[1;32m----> 4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39msapmle.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[0;32m      5\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(csvfile, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, quotechar\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mnext\u001b[39m(reader, \u001b[39mNone\u001b[39;00m) \u001b[39m# Skip header\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sapmle.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "X, y2 = [], []\n",
    "with open('sapmle.csv', 'rt') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    next(reader, None) # Skip header\n",
    "    \n",
    "    for row in reader:      \n",
    "        y2.append(row[1])\n",
    "        X.append(row[2])\n",
    "\n",
    "y_real = []        \n",
    "for i in y2:\n",
    "    y_real.append(int(i))\n",
    "\n",
    "# Making vector y one_hot\n",
    "y = [] # one hot y\n",
    "for i in range(len(y_real)):\n",
    "    if y_real[i] == 0:\n",
    "        y.append([1, 0])\n",
    "    else:\n",
    "        y.append([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of 0 and 1 classes\n",
    "pozitivan, negativan = 0, 0\n",
    "\n",
    "for y_ch in y_real:\n",
    "    if y_ch == 0:\n",
    "        negativan += 1\n",
    "    else:\n",
    "        pozitivan += 1\n",
    "\n",
    "print('br. pozitivnih: ', pozitivan, '\\nbr. negativnih: ', negativan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validtaiton and test set\n",
    "\n",
    "train_len = int((len(X)/20) * 0.6)\n",
    "validatioon_len = int((len(X)/20)*0.2 + train_len)\n",
    "test_len = 2 * validatioon_len\n",
    "\n",
    "X_train = X[:train_len]\n",
    "Y_train = y[:train_len]\n",
    "\n",
    "X_valid = X[train_len:validatioon_len]\n",
    "Y_valid = y[train_len:validatioon_len]\n",
    "\n",
    "X_test = X[validatioon_len:test_len]\n",
    "Y_test = y[validatioon_len:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sample\n",
    "for i in range(3):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences to tokens\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "X_train_sent = []\n",
    "X_valid_sent = []\n",
    "X_test_sent = []\n",
    "\n",
    "def split_to_sent(sent_array, x_array):\n",
    "    for i in range(len(x_array)):\n",
    "        sent_array.append(sent_tokenize(x_array[i]))\n",
    "\n",
    "split_to_sent(X_train_sent, X_train)\n",
    "split_to_sent(X_valid_sent, X_valid)\n",
    "split_to_sent(X_test_sent, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_sent[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences into words\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "X_train_word, X_valid_word, X_test_word = [], [], []\n",
    "\n",
    "# Funkcija za pronalazenje svih pozicija karaktera ch u stringu s\n",
    "def findOccurences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "def clean_data(data_set_to_split, data_set):\n",
    "    \"\"\"\n",
    "    data_set_to_split - set with sentences to split into words\n",
    "    data_set - set with words\n",
    "    \"\"\"\n",
    "    \n",
    "    line = [] # jedan twit\n",
    "    occurences = [] # lista sa pozicijama karaktera '@' u datoj recenici\n",
    "    occurences_and = [] # lista sa pozicijama karaktera '&' u datoj recenici\n",
    "    http_index = [] # lista sa pozicijama podstringa 'http' u datoj recenici\n",
    "    usernames = [] # list of usernames and strings starting with '&' to remove\n",
    "    http_list = [] # list of links to remove\n",
    "    \n",
    "    for x in data_set_to_split:\n",
    "        duzina = len(x)\n",
    "        for i in range(duzina):\n",
    "            string = str(x[i]).strip()\n",
    "            \n",
    "            # Remove usernames and links\n",
    "            occurences = findOccurences(string, '@')\n",
    "            occurences_and = findOccurences(string, '&')\n",
    "            http_index = [m.start() for m in re.finditer('(?=http)', string)]\n",
    "            \n",
    "            if occurences or occurences_and or http_index: # if any of the lists is not empty\n",
    "                if occurences_and:\n",
    "                    for index in occurences_and: # indexes of '&'\n",
    "                        stop_index = string.find(' ' or '\\n', index) # finds the first occurence of ' ' or '\\n'\n",
    "                        char_and = str(string[index:stop_index])\n",
    "                        usernames.append(char_and)\n",
    "                    occurences_and = []\n",
    "                if occurences:\n",
    "                    for index in occurences: # indexes of '@'\n",
    "                        stop_index = string.find(' ' or '\\n', index)\n",
    "                        user_name = str(string[index:stop_index]) # find twitter username: @blah\n",
    "                        usernames.append(user_name)\n",
    "                    occurences = []\n",
    "                if http_index:\n",
    "                    for index in http_index:\n",
    "                        stop_index = string.find(' ' or '\\n', index)\n",
    "                        link = str(string[index:stop_index])\n",
    "                        http_list.append(link)\n",
    "                    http_index = []\n",
    "\n",
    "                for username_link in usernames or http_list:\n",
    "                    if username_link in string:\n",
    "                        string = string.replace(username_link, '')\n",
    "                usernames = []\n",
    "                http_list = []\n",
    "            line.extend(regexp_tokenize(string, \"[\\w']+\"))\n",
    "        data_set.append(line)\n",
    "        line = []    \n",
    "\n",
    "## Train\n",
    "clean_data(X_train_sent, X_train_word)\n",
    "    \n",
    "## Validation\n",
    "clean_data(X_valid_sent, X_valid_word)\n",
    "\n",
    "## Test\n",
    "clean_data(X_test_sent, X_test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_word[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get a list of stopwords for english\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "stopwords_punctuation_list = set(stopword_list).union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and words that doesn't give huge meaning to sentence\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "\n",
    "X_final_train = []\n",
    "X_final_valid = []\n",
    "X_final_test = []\n",
    "\n",
    "def token(word_array, final_array):\n",
    "    for x in word_array:\n",
    "        x = [w.lower() for w in x if w not in stopwords_punctuation_list and not w.isdigit() and len(w) > 1 and not w[0].isdigit() and len(w) > 2]\n",
    "        final_array.append(x)\n",
    "        \n",
    "token(X_train_word, X_final_train)\n",
    "token(X_valid_word, X_final_valid)\n",
    "token(X_test_word, X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_train[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing words on the roots\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter = LancasterStemmer()\n",
    "\n",
    "def trim_to_root(final_array):\n",
    "    for x in final_array:\n",
    "        duzina = len(x)\n",
    "        for i in range(duzina):\n",
    "            x[i] = porter.stem(x[i])\n",
    "\n",
    "trim_to_root(X_final_train)\n",
    "trim_to_root(X_final_valid)\n",
    "trim_to_root(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_train[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding words from the training set to the dictionary and determining whether the word is 'positive' or 'negative'\n",
    "\n",
    "class_val = 0\n",
    "vrednosti = {}\n",
    "\n",
    "for x in X_final_train:\n",
    "    duzina_x = len(x)\n",
    "    for i in range(duzina_x):\n",
    "        if x[i] not in vrednosti:\n",
    "            vrednosti.setdefault(x[i], 0)\n",
    "        else:\n",
    "            if Y_train[class_val][0] == 1 and Y_train[class_val][1] == 0: # negativan[0,1], [1,0]\n",
    "                vrednosti[x[i]] -= 1\n",
    "            else:\n",
    "                vrednosti[x[i]] += 1\n",
    "    class_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vrednosti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing each tweet in the set via a nested pair [pos, neg]\n",
    "# [pos, neg] - where pos is the number of positive words in the tweet, and neg is the number of negative words in the tweet\n",
    "X_train_cor = [] # X sa koordinatama\n",
    "X_valid_cor = []\n",
    "X_test_cor = []\n",
    "\n",
    "def assign_coord(final_array, cor_array):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for x in final_array:\n",
    "        duzina_x = len(x)\n",
    "        for i in range(duzina_x):\n",
    "            if x[i] not in vrednosti:\n",
    "                pass\n",
    "            elif vrednosti[x[i]] == 0:\n",
    "                pass\n",
    "            elif vrednosti[x[i]] > 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "        cor_array.append([pos, neg])\n",
    "        pos, neg = 0, 0\n",
    "\n",
    "assign_coord(X_final_train, X_train_cor)\n",
    "assign_coord(X_final_valid, X_valid_cor)\n",
    "assign_coord(X_final_test, X_test_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training and validation, tweets for which we cannot determine the class are not taken\n",
    "# Deleting a tweet whose value is [0,0], there are no positive or negative words\n",
    "X_train_final, X_valid_final, X_test_final = X_train_cor, [], []\n",
    "\n",
    "def clear_tweet(coord_array, non_zero_array):\n",
    "    for x in coord_array:\n",
    "        if x[0] != 0 and x[1] != 0:\n",
    "            non_zero_array.append(x)\n",
    "\n",
    "clear_tweet(X_valid_cor, X_valid_final)\n",
    "clear_tweet(X_test_cor, X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_valid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the algorithm will go through k's from k_valid and remember the best precision in acc_valid and k in K\n",
    "k_valid = (3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15) # hide even numbers for this data set\n",
    "K_valid = 0 # k from k_valid that gives the most colorful results will become K_valid\n",
    "acc_valid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input train vector\n",
    "x1 = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "\n",
    "# input validation/test vector\n",
    "x2 = tf.placeholder(dtype=tf.float32, shape=[2])\n",
    "\n",
    "# number of classes k\n",
    "K = tf.placeholder(dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate L2 norm\n",
    "\n",
    "# Euclidean distance\n",
    "distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x1, x2)), axis=1)) # 300x1\n",
    "# weighted distance\n",
    "w_distance = 1.0/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = tf.nn.top_k(tf.negative(w_distance), K)\n",
    "\n",
    "k_nn_labels = tf.gather(Y_train, indices) # returns a vector of nicely mapped labels with their indices\n",
    "\n",
    "predict = tf.argmax(tf.reduce_sum(k_nn_labels, axis=0), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    for k in k_valid:\n",
    "        \n",
    "        print('Validatioin for k = ', k)\n",
    "        \n",
    "        accuracy = 0.0\n",
    "\n",
    "        for i in range(len(X_valid_final)):\n",
    "\n",
    "        # of us we found\n",
    "            pred_y = sess.run(predict, feed_dict={x1: X_train_final, x2: X_valid_final[i], K:k})\n",
    "\n",
    "            # check if pred_y is a number\n",
    "            if not pred_y.dtype == 'int64':\n",
    "                pred_y = Y_train[i][tf.reduce_max(Y_train[i], axis=0).eval()]\n",
    "            \n",
    "           # real class\n",
    "\n",
    "            true_y = tf.argmax(Y_valid[i], axis=0).eval() # eval vrati poziciju najveceg elementa u Y_np_valid\n",
    "\n",
    "            match = pred_y == true_y\n",
    "\n",
    "            print(\"[Validation %3d] Prediction: %d, True Class: %d, Match: %d\" % (i, pred_y, true_y, match))\n",
    "            \n",
    "            #print('X_np_train ', X_np_train[i], ' klasa: ', Y_train[i])\n",
    "            #print('shape: ', )\n",
    "            #print('\\ndistanca: \\n', sess.run(distance, feed_dict={x1: X_np_train, x2: X_np_valid[i], K:k}))\n",
    "            #print('\\nw_distanca: \\n', sess.run(w_distance, feed_dict={x1: X_np_train, x2: X_np_valid[i], K:k}))\n",
    "            #print('*'*25)\n",
    "\n",
    "            \n",
    "            if match:\n",
    "                accuracy += 1.0 / len(X_valid_final)\n",
    "\n",
    "            if accuracy > acc_valid:\n",
    "                acc_valid = accuracy\n",
    "                K_valid = k\n",
    "\n",
    "        print('accuracy for k = ', k, ' -> ', accuracy)\n",
    "    print('The best accuracy', acc_valid, ' is with k = ', K_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy with best k from validatin in test set\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(X_test_final)):\n",
    "\n",
    "        # of us we found\n",
    "\n",
    "        pred_y = sess.run(predict, feed_dict={x1: X_train_final, x2: X_test_final[i], K:K_valid})\n",
    "\n",
    "        # check if pred_y is a number, if not set it to be of the class for which distance got 0 # isdigit()\n",
    "        if not pred_y.dtype == 'int64':\n",
    "            pred_y = Y_train[i][tf.reduce_max(Y_train[i], axis=0).eval()]\n",
    "\n",
    "        # real class\n",
    "        true_y = tf.argmax(Y_train[i], axis=0).eval() #Y_np_test should be one_hot\n",
    "\n",
    "        match = pred_y == true_y\n",
    "\n",
    "        print(\"[Test %3d] Prediction: %d, True Class: %d, Match: %d\" % (i, pred_y, true_y, match))\n",
    "\n",
    "        if match:\n",
    "            accuracy += 1.0 / len(X_test_final)\n",
    "\n",
    "    print('For k = %d accuaracy on:' % (K_valid))\n",
    "    print(' - validation set is ', acc_valid)\n",
    "    print(' - test set is ', accuracy)\n",
    "\n",
    "    print('Difference in accuracy: ', acc_valid - accuracy) if acc_valid > accuracy else print('difference in accuracy: ', accuracy - acc_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
